{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Set the video capture properties\n",
    "camera_index = 0  # Change this if you have multiple cameras\n",
    "frame_width = 640\n",
    "frame_height = 480\n",
    "fps = 30\n",
    "video_format = 'XVID'\n",
    "video_file_path = '/Users/thinnaphatd/Dev_work/deep_learning/Image/video2.mp4'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "output_directory = os.path.dirname(video_file_path)\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Initialize the camera\n",
    "cap = cv2.VideoCapture(camera_index)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, frame_width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, frame_height)\n",
    "cap.set(cv2.CAP_PROP_FPS, fps)\n",
    "\n",
    "# Initialize the video writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*video_format)\n",
    "out = cv2.VideoWriter(video_file_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "print(\"Press 'q' to stop recording and save the video.\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    # Capture a frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        # Write the frame to the output file\n",
    "        out.write(frame)\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Camera', frame)\n",
    "\n",
    "        # Check if 'q' key is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release the camera and video writer, and close the display window\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Video saved to {video_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def capture_roi(video_file_path, output_image_path, roi_coordinates, frame_number):\n",
    "    cap = cv2.VideoCapture(video_file_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    if frame_number >= frame_count:\n",
    "        print(f\"Frame number {frame_number} is out of range. The video has {frame_count} frames.\")\n",
    "        return\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        x, y, w, h = roi_coordinates\n",
    "        roi = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        cv2.imwrite(output_image_path, roi)\n",
    "        print(f\"Saved ROI image to {output_image_path}\")\n",
    "    else:\n",
    "        print(f\"Error reading frame {frame_number}\")\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "video_file_path = '/Users/thinnaphatd/Dev_work/deep_learning/Image/video2.mp4'  # Change this to your video file path\n",
    "output_image_path = '/Users/thinnaphatd/Dev_work/deep_learning/Image/roi_image5.jpg'\n",
    "roi_coordinates = (0, 0, 640, 480)  # Replace these with the coordinates (x, y) and dimensions (w, h) of the desired ROI\n",
    "frame_number = 527  # Change this to the desired frame number\n",
    "\n",
    "capture_roi(video_file_path, output_image_path, roi_coordinates, frame_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Match contours to license plate or character template\n",
    "def find_contours(dimensions, img) :\n",
    "\n",
    "    # Find all contours in the image\n",
    "    cntrs, _ = cv2.findContours(img.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Retrieve potential dimensions\n",
    "    lower_width = dimensions[0]\n",
    "    upper_width = dimensions[1]\n",
    "    lower_height = dimensions[2]\n",
    "    upper_height = dimensions[3]\n",
    "    \n",
    "    # Check largest 5 or  15 contours for license plate or character respectively\n",
    "    cntrs = sorted(cntrs, key=cv2.contourArea, reverse=True)[:15]\n",
    "    \n",
    "    ii = cv2.imread('/Users/thinnaphatd/Dev_work/deep_learning/Image/roi_image5.jpg')\n",
    "    \n",
    "    \n",
    "    x_cntr_list = []\n",
    "    target_contours = []\n",
    "    img_res = []\n",
    "    for cntr in cntrs :\n",
    "        #detects contour in binary image and returns the coordinates of rectangle enclosing it\n",
    "        intX, intY, intWidth, intHeight = cv2.boundingRect(cntr)\n",
    "        \n",
    "        #checking the dimensions of the contour to filter out the characters by contour's size\n",
    "        if intWidth > lower_width and intWidth < upper_width and intHeight > lower_height and intHeight < upper_height :\n",
    "            x_cntr_list.append(intX) #stores the x coordinate of the character's contour, to used later for indexing the contours\n",
    "\n",
    "            char_copy = np.zeros((44,24))\n",
    "            #extracting each character using the enclosing rectangle's coordinates.\n",
    "            char = img[intY:intY+intHeight, intX:intX+intWidth]\n",
    "            char = cv2.resize(char, (20, 40))\n",
    "            \n",
    "            cv2.rectangle(ii, (intX,intY), (intWidth+intX, intY+intHeight), (50,21,200), 2)\n",
    "            plt.imshow(ii, cmap='gray')\n",
    "\n",
    "#             Make result formatted for classification: invert colors\n",
    "            char = cv2.subtract(255, char)\n",
    "\n",
    "            # Resize the image to 24x44 with black border\n",
    "            char_copy[2:42, 2:22] = char\n",
    "            char_copy[0:2, :] = 0\n",
    "            char_copy[:, 0:2] = 0\n",
    "            char_copy[42:44, :] = 0\n",
    "            char_copy[:, 22:24] = 0\n",
    "\n",
    "            img_res.append(char_copy) #List that stores the character's binary image (unsorted)\n",
    "            \n",
    "    #Return characters on ascending order with respect to the x-coordinate (most-left character first)\n",
    "            \n",
    "    plt.show()\n",
    "    #arbitrary function that stores sorted list of character indeces\n",
    "    indices = sorted(range(len(x_cntr_list)), key=lambda k: x_cntr_list[k])\n",
    "    img_res_copy = []\n",
    "    for idx in indices:\n",
    "        img_res_copy.append(img_res[idx])# stores character images according to their index\n",
    "    img_res = np.array(img_res_copy)\n",
    "\n",
    "    return img_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find characters in the resulting images\n",
    "def segment_characters(image) :\n",
    "\n",
    "    # Preprocess cropped license plate image\n",
    "    img_lp = cv2.resize(image, (300, 75))\n",
    "    img_gray_lp = cv2.cvtColor(img_lp, cv2.COLOR_BGR2GRAY)\n",
    "    _, img_binary_lp = cv2.threshold(img_gray_lp, 200, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    img_binary_lp = cv2.erode(img_binary_lp, (3,3))\n",
    "    img_binary_lp = cv2.dilate(img_binary_lp, (3,3))\n",
    "\n",
    "    LP_WIDTH = img_binary_lp.shape[0]\n",
    "    LP_HEIGHT = img_binary_lp.shape[1]\n",
    "\n",
    "    # Make borders white\n",
    "    img_binary_lp[0:3,:] = 255\n",
    "    img_binary_lp[:,0:3] = 255\n",
    "    img_binary_lp[72:75,:] = 255\n",
    "    img_binary_lp[:,330:333] = 255\n",
    "\n",
    "    # Estimations of character contours sizes of cropped license plates\n",
    "    dimensions = [LP_WIDTH/6,\n",
    "                       LP_WIDTH/2,\n",
    "                       LP_HEIGHT/10,\n",
    "                       2*LP_HEIGHT/3]\n",
    "    plt.imshow(img_binary_lp, cmap='gray')\n",
    "    plt.show()\n",
    "    cv2.imwrite('contour.jpg',img_binary_lp)\n",
    "\n",
    "    # Get contours within cropped license plate\n",
    "    char_list = find_contours(dimensions, img_binary_lp)\n",
    "\n",
    "    return char_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('/Users/thinnaphatd/Dev_work/deep_learning/Image/roi_image5.jpg')\n",
    "char = segment_characters(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the pre-trained Keras model\n",
    "model = load_model('/Users/thinnaphatd/Dev_work/deep_learning/ENGcharacter_weightFinal.h5')\n",
    "\n",
    "# Define the video file path\n",
    "video_file_path = '/Users/thinnaphatd/Dev_work/deep_learning/Image/video1.avi'\n",
    "\n",
    "# Initialize the video capture\n",
    "cap = cv2.VideoCapture(video_file_path)\n",
    "\n",
    "# Define the preprocessing function (depending on your model)\n",
    "def preprocess_frame(frame):\n",
    "    # Resize the frame, normalize the pixel values, and expand the dimensions\n",
    "    frame_resized = cv2.resize(frame, (28, 28))  # Change the size according to your model's input\n",
    "    frame_normalized = frame_resized.astype('float32') / 255.0\n",
    "    frame_expanded = np.expand_dims(frame_normalized, axis=0)\n",
    "    return frame_expanded\n",
    "\n",
    "# Process the video frames and make predictions\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        def fix_dimension(img): \n",
    "             new_img = np.zeros((28,28,3))\n",
    "             for i in range(3):\n",
    "                  new_img[:,:,i] = img\n",
    "             return new_img\n",
    "        # Preprocess the frame\n",
    "        processed_frame = preprocess_frame(frame)\n",
    "\n",
    "        # Make a prediction using the model\n",
    "        # prediction = model.predict(processed_frame)\n",
    "\n",
    "        # Process the prediction (e.g., print the predicted class)\n",
    "        def show_results():\n",
    "            dic = {}\n",
    "            characters = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "            for i,c in enumerate(characters):\n",
    "                  dic[i] = c\n",
    "            output = []\n",
    "            for i,ch in enumerate(char): #iterating over the characters\n",
    "                img_ = cv2.resize(ch, (28,28), interpolation=cv2.INTER_AREA)\n",
    "                img = fix_dimension(img_)\n",
    "                img = img.reshape(1,28,28,3) #preparing image for the model\n",
    "                y_ = model.predict(img)[0] #predicting the class\n",
    "                for num in range(len(y_)):\n",
    "                    if y_[num] == 1:\n",
    "                        y_num = num\n",
    "#         print(img)\n",
    "#         print(y_)\n",
    "#         print(dic)\n",
    "        \n",
    "                character = dic[y_num] \n",
    "#         print(character)\n",
    "                output.append(character) #storing the result in a list\n",
    "        \n",
    "        \n",
    "        \n",
    "            plate_number = ''.join(output)\n",
    "    \n",
    "            return plate_number\n",
    "\n",
    "        print(show_results())\n",
    "\n",
    "        # predicted_class = np.argmax(prediction)\n",
    "        # print(f\"Predicted class: {predicted_class}\")\n",
    "\n",
    "#         # Display the frame\n",
    "#         cv2.imshow('Video', frame)\n",
    "\n",
    "#         # Check if 'q' key is pressed\n",
    "#         if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "#             break\n",
    "#     else:\n",
    "#         break\n",
    "\n",
    "# # Release the video capture and close the display window\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39m# Perform object detection\u001b[39;00m\n\u001b[1;32m     22\u001b[0m net\u001b[39m.\u001b[39msetInput(blob)\n\u001b[0;32m---> 23\u001b[0m detections \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39;49mforward()\n\u001b[1;32m     25\u001b[0m \u001b[39m# Process detections\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(detections\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_weights = \"/Users/thinnaphatd/Dev_work/deep_learning/resnet10_fd_lpd.caffemodel\"\n",
    "model_config = \"/Users/thinnaphatd/Dev_work/deep_learning/resnet10_fd_lpd.prototxt\"\n",
    "net = cv2.dnn.readNetFromCaffe(model_config, model_weights)\n",
    "\n",
    "# Set up camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read the current frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Prepare the frame for object detection\n",
    "    h, w = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "\n",
    "    # Perform object detection\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    # Process detections\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > 0.5:  # Set the confidence threshold\n",
    "            class_id = int(detections[0, 0, i, 1])\n",
    "\n",
    "            # Check if detected object is a license plate\n",
    "            if class_id == 1:\n",
    "                box = detections[0, 0, i, 3:7] * [w, h, w, h]\n",
    "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "                cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "\n",
    "    # Show the result\n",
    "    cv2.imshow(\"License Plate Detection\", frame)\n",
    "\n",
    "    # Exit the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_weights = \"/Users/thinnaphatd/Dev_work/deep_learning/resnet10_fd_lpd.caffemodel\"\n",
    "model_config = \"/Users/thinnaphatd/Dev_work/deep_learning/resnet10_fd_lpd.prototxt\"\n",
    "net = cv2.dnn.readNetFromCaffe(model_config, model_weights)\n",
    "\n",
    "# Set up camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read the current frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Prepare the frame for object detection\n",
    "    h, w = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "\n",
    "    # Perform object detection\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    # Process detections\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > 0.2:  # Set the confidence threshold\n",
    "            class_id = int(detections[0, 0, i, 1])\n",
    "\n",
    "            # Check if detected object is a license plate\n",
    "            if class_id == 1:\n",
    "                box = detections[0, 0, i, 3:7] * [w, h, w, h]\n",
    "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "                cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "\n",
    "    # Show the result\n",
    "    cv2.imshow(\"License Plate Detection\", frame)\n",
    "\n",
    "    # Exit the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2155e09a7e62474c8869d4d274d421c18540fc38d064f79466f005889818f3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
